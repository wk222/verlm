# verl æ¡†æ¶ ADPO/GRPO/PPO è®­ç»ƒ Batch Size å‚æ•°å®Œå…¨æŒ‡å—

> æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Šè®­ç»ƒè„šæœ¬ä¸­å„å‚æ•°çš„å«ä¹‰ã€ç›¸äº’å…³ç³»ã€çº¦æŸæ¡ä»¶ï¼Œä»¥åŠå¯¹æ˜¾å­˜å’Œé€Ÿåº¦çš„å½±å“ã€‚

---

## ğŸ“Š å‚æ•°ä¸€è§ˆè¡¨

| å‚æ•° | é»˜è®¤å€¼ | æ˜¾å­˜å½±å“ | é€Ÿåº¦å½±å“ | è¯´æ˜ |
|------|--------|----------|----------|------|
| `train_batch_size` | 1024 | â­ | â­â­â­â­ | æ¯æ­¥å¤„ç†çš„ prompt æ•°é‡ |
| `rollout.n` | 1 | â­â­â­â­ | â­â­â­â­ | æ¯ä¸ª prompt ç”Ÿæˆçš„ response æ•°é‡ |
| `ppo_mini_batch_size` | 256 | â­â­â­ | â­â­â­ | PPO æ›´æ–°æ—¶çš„å…¨å±€ mini-batch å¤§å° |
| `ppo_micro_batch_size_per_gpu` | null | â­â­ | â­â­ | å• GPU forward/backward æ‰¹å¤§å° |
| `log_prob_micro_batch_size_per_gpu` | null | â­â­ | â­ | log prob è®¡ç®—æ—¶å• GPU æ‰¹å¤§å° |
| `gpu_memory_utilization` | 0.5 | â­â­â­â­â­ | â­â­â­ | vLLM KV cache é¢„åˆ†é…æ¯”ä¾‹ |
| `max_num_seqs` | 1024 | â­â­ | â­â­ | vLLM æœ€å¤§å¹¶å‘åºåˆ—æ•° |
| `max_prompt_length` | - | â­â­â­â­ | â­â­â­â­ | æœ€å¤§ prompt é•¿åº¦ |
| `max_response_length` | - | â­â­â­â­â­ | â­â­â­â­â­ | æœ€å¤§ response é•¿åº¦ |

---

## ğŸ”— å‚æ•°å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           æ•°æ®æµ                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   train_batch_size (64 prompts)                                     â”‚
â”‚          â”‚                                                          â”‚
â”‚          â†“                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚   â”‚  Ã— rollout.n (8)  â”‚  â† æ¯ä¸ª prompt ç”Ÿæˆ n ä¸ª response           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚          â†“                                                          â”‚
â”‚   real_train_batch_size = 64 Ã— 8 = 512 responses                    â”‚
â”‚          â”‚                                                          â”‚
â”‚          â†“                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚              Rollout é˜¶æ®µ (vLLM/SGLang)                  â”‚      â”‚
â”‚   â”‚  gpu_memory_utilization=0.35 â†’ KV cache é¢„åˆ†é…           â”‚      â”‚
â”‚   â”‚  max_num_seqs=192 â†’ æœ€å¤§å¹¶å‘æ•°                           â”‚      â”‚
â”‚   â”‚  log_prob_micro_batch_size_per_gpu=16 â†’ log prob æ‰¹æ¬¡    â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                                                          â”‚
â”‚          â†“                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚              Training é˜¶æ®µ (Actor Update)                â”‚      â”‚
â”‚   â”‚                                                          â”‚      â”‚
â”‚   â”‚  ppo_mini_batch_size=32                                  â”‚      â”‚
â”‚   â”‚       â†“ (Ã— rollout.n / n_gpus)                          â”‚      â”‚
â”‚   â”‚  normalized_mini_batch = 32 Ã— 8 / 4 = 64 per GPU        â”‚      â”‚
â”‚   â”‚       â†“                                                  â”‚      â”‚
â”‚   â”‚  ppo_micro_batch_size_per_gpu=8                          â”‚      â”‚
â”‚   â”‚       â†“                                                  â”‚      â”‚
â”‚   â”‚  gradient_accumulation_steps = 64 / 8 = 8 æ­¥             â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ å‚æ•°è¯¦è§£

### 1. `data.train_batch_size`

**å®šä¹‰**ï¼šæ¯ä¸ªè®­ç»ƒæ­¥éª¤å¤„ç†çš„ **prompt æ•°é‡**ã€‚

**å½±å“**ï¼š
- â¬† å¢å¤§ï¼šæ¯æ­¥å¤„ç†æ›´å¤šæ•°æ®ï¼Œæ€»æ­¥æ•°å‡å°‘ï¼Œä½†æ˜¾å­˜å³°å€¼ç•¥å¢
- â¬‡ å‡å°ï¼šæ¯æ­¥å¤„ç†æ›´å°‘æ•°æ®ï¼Œæ€»æ­¥æ•°å¢åŠ ï¼Œæ˜¾å­˜é™ä½

**è®¡ç®—å…³ç³»**ï¼š
```python
real_train_batch_size = train_batch_size Ã— rollout.n
# è¿™æ˜¯å®é™…çš„ response æ€»æ•°
```

---

### 2. `actor_rollout_ref.rollout.n`

**å®šä¹‰**ï¼šæ¯ä¸ª prompt ç”Ÿæˆçš„ response æ•°é‡ï¼ˆé‡‡æ ·æ¬¡æ•°ï¼‰ã€‚

**å…³é”®çº¦æŸ**ï¼š
- **GRPO/ADPO å¿…é¡» > 1**ï¼ˆéœ€è¦ç»„å†…å¯¹æ¯”ï¼‰
- PPO é€šå¸¸ = 1

**å½±å“**ï¼š
- â¬† å¢å¤§ï¼šç”Ÿæˆæ›´å¤šæ ·æœ¬ï¼Œrollout é˜¶æ®µæ˜¾å­˜å’Œæ—¶é—´çº¿æ€§å¢åŠ 
- â¬‡ å‡å°ï¼šæ ·æœ¬å¤šæ ·æ€§é™ä½

**ä¸å…¶ä»–å‚æ•°çš„å…³ç³»**ï¼š
```python
# æ€»æ ·æœ¬æ•°
total_responses = train_batch_size Ã— rollout.n

# normalized_mini_batch_size è®¡ç®—
normalized_mini_batch = ppo_mini_batch_size Ã— rollout.n / n_gpus
```

---

### 3. `actor_rollout_ref.actor.ppo_mini_batch_size`

**å®šä¹‰**ï¼šPPO æ›´æ–°æ—¶çš„**å…¨å±€** mini-batch å¤§å°ï¼ˆresponse æ•°é‡ï¼‰ã€‚

**çº¦æŸæ¡ä»¶**ï¼š
```python
# å¿…é¡»æ»¡è¶³
ppo_mini_batch_size â‰¤ train_batch_size

# Worker å†…éƒ¨å½’ä¸€åŒ–
normalized = ppo_mini_batch_size Ã— rollout.n / n_gpus
# å¿…é¡»æ»¡è¶³
normalized % ppo_micro_batch_size_per_gpu == 0
```

**å½±å“**ï¼š
- â¬† å¢å¤§ï¼šæ¯æ¬¡æ›´æ–°ä½¿ç”¨æ›´å¤šæ ·æœ¬ï¼Œæ¢¯åº¦æ›´ç¨³å®š
- â¬‡ å‡å°ï¼šæ›´é¢‘ç¹æ›´æ–°ï¼Œå¯èƒ½æ›´å¿«æ”¶æ•›ä½†å™ªå£°æ›´å¤§

---

### 4. `actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu`

**å®šä¹‰**ï¼šå• GPU ä¸Šæ¯æ¬¡ forward/backward çš„æ ·æœ¬æ•°ã€‚ç”¨äº**æ¢¯åº¦ç´¯ç§¯**ã€‚

**çº¦æŸæ¡ä»¶**ï¼š
```python
# å¿…é¡»æ•´é™¤ normalized_mini_batch_size
normalized_mini_batch % ppo_micro_batch_size_per_gpu == 0
```

**æ¢¯åº¦ç´¯ç§¯æ­¥æ•°è®¡ç®—**ï¼š
```python
gradient_accumulation_steps = normalized_mini_batch / ppo_micro_batch_size_per_gpu
```

**å½±å“**ï¼š
- â¬† å¢å¤§ï¼šæ¯æ¬¡å¤„ç†æ›´å¤šï¼Œè®­ç»ƒæ›´å¿«ï¼Œä½†æ˜¾å­˜å³°å€¼æ›´é«˜
- â¬‡ å‡å°ï¼šæ˜¾å­˜é™ä½ï¼Œä½†æ¢¯åº¦ç´¯ç§¯æ­¥æ•°å¢åŠ ï¼Œè®­ç»ƒå˜æ…¢

**ç¤ºä¾‹**ï¼š
| ppo_micro_batch_size_per_gpu | normalized_mini_batch | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | æ˜¾å­˜ | é€Ÿåº¦ |
|------------------------------|----------------------|--------------|------|------|
| 8 | 64 | 8 | é«˜ | å¿« |
| 4 | 64 | 16 | ä¸­ | ä¸­ |
| 2 | 64 | 32 | ä½ | æ…¢ |

---

### 5. `actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu`

**å®šä¹‰**ï¼šè®¡ç®— old log probability æ—¶çš„å• GPU æ‰¹å¤§å°ã€‚

**æ— ä¸¥æ ¼æ•´é™¤çº¦æŸ**ï¼Œå¯è‡ªç”±è°ƒæ•´ã€‚

**å½±å“**ï¼š
- ä»…å½±å“ log prob è®¡ç®—é˜¶æ®µçš„æ˜¾å­˜
- å¯¹è®­ç»ƒé€Ÿåº¦å½±å“è¾ƒå°ï¼ˆé€šå¸¸ log prob è®¡ç®—å¾ˆå¿«ï¼‰

---

### 6. `actor_rollout_ref.rollout.gpu_memory_utilization`

**å®šä¹‰**ï¼švLLM å¼•æ“é¢„åˆ†é… GPU æ˜¾å­˜çš„æ¯”ä¾‹ã€‚

**å–å€¼èŒƒå›´**ï¼š0.0 ~ 1.0ï¼ˆæ¨è 0.35 ~ 0.7ï¼‰

**å½±å“**ï¼š
- â¬† å¢å¤§ï¼šæ›´å¤§çš„ KV cacheï¼Œrollout æ›´å¿«ï¼Œä½†å¯èƒ½æŒ¤å è®­ç»ƒæ˜¾å­˜
- â¬‡ å‡å°ï¼šrollout å˜æ…¢ï¼Œä½†ç»™è®­ç»ƒé˜¶æ®µç•™æ›´å¤šæ˜¾å­˜

**é‡è¦**ï¼šæ­¤å‚æ•°åªå½±å“ **rollout é˜¶æ®µ**ï¼Œè®¾ç½® `free_cache_engine=True` åè®­ç»ƒå‰ä¼šé‡Šæ”¾ã€‚

---

### 7. `actor_rollout_ref.rollout.max_num_seqs`

**å®šä¹‰**ï¼švLLM å¼•æ“åŒæ—¶å¤„ç†çš„æœ€å¤§åºåˆ—æ•°ã€‚

**å½±å“**ï¼š
- â¬† å¢å¤§ï¼šæ›´é«˜å¹¶å‘ï¼Œæ›´å¥½çš„ GPU åˆ©ç”¨ç‡
- â¬‡ å‡å°ï¼šé™ä½å¹¶å‘ï¼Œå¯èƒ½é™ä½åå

---

## âš–ï¸ çº¦æŸæ¡ä»¶æ±‡æ€»

### æ ¸å¿ƒçº¦æŸå…¬å¼

```python
# 1. æ€»æ ·æœ¬æ•°å¿…é¡»èƒ½è¢« GPU æ•°æ•´é™¤
(train_batch_size Ã— rollout.n) % n_gpus == 0

# 2. normalized_mini_batch å¿…é¡»èƒ½è¢« micro_batch æ•´é™¤
normalized_mini_batch = ppo_mini_batch_size Ã— rollout.n / n_gpus
normalized_mini_batch % ppo_micro_batch_size_per_gpu == 0

# 3. mini_batch_size ä¸èƒ½è¶…è¿‡ train_batch_size
ppo_mini_batch_size â‰¤ train_batch_size
```

### 4 GPU é…ç½®éªŒè¯ç¤ºä¾‹

å½“å‰é…ç½®ï¼š
- `train_batch_size=64`
- `rollout.n=8`
- `ppo_mini_batch_size=32`
- `ppo_micro_batch_size_per_gpu=8`
- `n_gpus=4`

éªŒè¯ï¼š
```python
# çº¦æŸ 1: (64 Ã— 8) % 4 == 512 % 4 == 0 âœ…
# çº¦æŸ 2: (32 Ã— 8 / 4) % 8 == 64 % 8 == 0 âœ…
# çº¦æŸ 3: 32 â‰¤ 64 âœ…

# æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 64 / 8 = 8 æ­¥
```

---

## ğŸ›ï¸ è°ƒå‚æŒ‡å—

### æ˜¾å­˜ä¸å¤Ÿï¼ŸæŒ‰ä¼˜å…ˆçº§è°ƒæ•´

1. **`max_response_length`** - é™åˆ° 1024 æˆ–æ›´ä½ï¼ˆå½±å“æœ€å¤§ï¼‰
2. **`gpu_memory_utilization`** - é™åˆ° 0.3~0.4
3. **`ppo_micro_batch_size_per_gpu`** - é™åˆ° 4 æˆ– 2
4. **`log_prob_micro_batch_size_per_gpu`** - é™åˆ° 8 æˆ– 4
5. **`train_batch_size`** å’Œ **`ppo_mini_batch_size`** - åŒæ¯”ä¾‹é™ä½

### é€Ÿåº¦å¤ªæ…¢ï¼ŸæŒ‰ä¼˜å…ˆçº§è°ƒæ•´

1. **`ppo_micro_batch_size_per_gpu`** - æé«˜ï¼ˆéœ€è¦æ˜¾å­˜æ”¯æŒï¼‰
2. **`gpu_memory_utilization`** - æé«˜åˆ° 0.5~0.6
3. **`log_prob_micro_batch_size_per_gpu`** - æé«˜åˆ° 16~32
4. **`max_num_seqs`** - æé«˜å¹¶å‘æ•°

---
max_response_length=1280	â­â­â­â­â­	â­â­â­â­â­	å½±å“æœ€å¤§ï¼åºåˆ—é•¿åº¦ç›´æ¥å½±å“ KV Cache å’Œæ¿€æ´»æ˜¾å­˜
max_prompt_length=880	â­â­â­â­	â­â­â­â­	ä¸ response_length ç±»ä¼¼ï¼Œä½†é€šå¸¸è¾ƒçŸ­
gpu_memory_utilization=0.45	â­â­â­â­	â­â­â­	vLLM KV Cache é¢„åˆ†é…æ¯”ä¾‹ï¼Œç›´æ¥æ§åˆ¶ rollout æ˜¾å­˜
rollout.n=8	â­â­â­â­	â­â­â­â­	æ¯ä¸ª prompt ç”Ÿæˆ n ä¸ªå“åº”ï¼Œæ˜¾å­˜å’Œè®¡ç®—é‡çº¿æ€§å¢é•¿
train_batch_size=128	â­â­â­	â­â­â­â­	æ€»ä½“æ‰¹æ¬¡å¤§å°ï¼Œé€šè¿‡æ¢¯åº¦ç´¯ç§¯åˆ†æ‘Š
ppo_mini_batch_size=64	â­â­â­	â­â­â­	æ¯æ¬¡æ›´æ–°çš„æ ·æœ¬æ•°ï¼Œå½±å“æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
ppo_micro_batch_size_per_gpu=8	â­â­	â­â­	å• GPU å‰å‘/åå‘æ‰¹æ¬¡ï¼Œé™ä½å¯å‡å°‘è®­ç»ƒå³°å€¼æ˜¾å­˜
log_prob_micro_batch_size_per_gpu=8	â­â­	â­	log prob è®¡ç®—æ‰¹æ¬¡ï¼Œä»…å½±å“è¯¥é˜¶æ®µæ˜¾å­˜
max_num_seqs=192	â­â­	â­â­	vLLM å¹¶å‘åºåˆ—æ•°ï¼Œå½±å“ rollout è°ƒåº¦
fsdp_config.param_offload=False	â­â­â­	â­â­â­	å¼€å¯å¯å¤§å¹…é™ä½æ˜¾å­˜ï¼Œä½†æ˜¾è‘—é™é€Ÿ
enforce_eager=False	â­	â­â­	True ç¦ç”¨ CUDA Graphï¼Œé™æ˜¾å­˜ä½†é™é€Ÿ
enable_chunked_prefill=True	â­	â­	åˆ†å— prefillï¼Œç•¥å¾®é™æ˜¾å­˜
enable_prefix_caching=True	â­	â­â­	å‰ç¼€ç¼“å­˜ï¼Œå¯èƒ½åŠ é€Ÿé‡å¤ prompt
free_cache_engine=True	â­â­	â­	rollout åé‡Šæ”¾ KV Cacheï¼Œé™æ˜¾å­˜ä½†æœ‰é‡å»ºå¼€é”€
## ğŸ”„ ADPO vs GRPO vs PPO å¯¹æ¯”

| ç‰¹æ€§ | PPO | GRPO | ADPO |
|------|-----|------|------|
| **Critic æ¨¡å‹** | âœ… éœ€è¦ | âŒ æ— éœ€ | âŒ æ— éœ€ |
| **æ˜¾å­˜å¼€é”€** | é«˜ï¼ˆåŒæ¨¡å‹ï¼‰ | ä½ | ä½ |
| **rollout.n è¦æ±‚** | é€šå¸¸ = 1 | **> 1** | **> 1** |
| **ä¼˜åŠ¿ä¼°è®¡å™¨** | GAE | Group-relative | Anchored softmax |

### GRPO/ADPO ç‰¹æ®Šè¦æ±‚

```yaml
# rollout.n å¿…é¡» > 1ï¼Œç”¨äºç»„å†…å¯¹æ¯”
actor_rollout_ref:
  rollout:
    n: 8  # å¿…é¡» > 1ï¼
    
algorithm:
  adv_estimator: grpo  # æˆ– adpo
  num_generations: 8   # ADPO éœ€è¦ï¼Œé€šå¸¸ç­‰äº rollout.n
```

---

## ğŸ“Š å®é™…é…ç½®ç¤ºä¾‹

### 4x4090 (24GB) ADPO é…ç½®

```bash
# ä¿å®ˆé…ç½®ï¼ˆç¨³å®šè¿è¡Œï¼‰
data.train_batch_size=64
data.max_prompt_length=880
data.max_response_length=1280
actor_rollout_ref.rollout.n=8
actor_rollout_ref.rollout.gpu_memory_utilization=0.35
actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16
actor_rollout_ref.actor.ppo_mini_batch_size=32
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8

# è®¡ç®—éªŒè¯
# real_batch = 64 Ã— 8 = 512, 512 % 4 = 0 âœ…
# normalized_mini = 32 Ã— 8 / 4 = 64, 64 % 8 = 0 âœ…
# grad_accum = 64 / 8 = 8 æ­¥
```

### 8xA100 (80GB) GRPO é«˜é€Ÿé…ç½®

```bash
data.train_batch_size=256
data.max_prompt_length=1024
data.max_response_length=2048
actor_rollout_ref.rollout.n=8
actor_rollout_ref.rollout.gpu_memory_utilization=0.7
actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32
actor_rollout_ref.actor.ppo_mini_batch_size=128
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16

# è®¡ç®—éªŒè¯
# real_batch = 256 Ã— 8 = 2048, 2048 % 8 = 0 âœ…
# normalized_mini = 128 Ã— 8 / 8 = 128, 128 % 16 = 0 âœ…
# grad_accum = 128 / 16 = 8 æ­¥
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æŠ¥é”™ `normalized ppo_mini_batch_size should be divisible by ppo_micro_batch_size_per_gpu`

**åŸå› **ï¼šçº¦æŸ 2 ä¸æ»¡è¶³ã€‚

**è§£å†³**ï¼šè°ƒæ•´ `ppo_micro_batch_size_per_gpu` ä½¿å…¶èƒ½æ•´é™¤ `normalized_mini_batch`ã€‚

```python
normalized = ppo_mini_batch_size Ã— rollout.n / n_gpus
# é€‰æ‹©èƒ½æ•´é™¤ normalized çš„å€¼
```

### Q2: OOM åœ¨ rollout é˜¶æ®µ

**è§£å†³**ï¼š
1. é™ä½ `gpu_memory_utilization` (å¦‚ 0.3)
2. é™ä½ `max_num_seqs` (å¦‚ 64)
3. å‡å° `max_response_length`

### Q3: OOM åœ¨ training é˜¶æ®µ

**è§£å†³**ï¼š
1. é™ä½ `ppo_micro_batch_size_per_gpu` (å¦‚ 2 æˆ– 1)
2. å¼€å¯ `gradient_checkpointing: true`
3. å¼€å¯ `fsdp_config.param_offload: true` (ä¼šå˜æ…¢)

### Q4: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**æ£€æŸ¥**ï¼š
1. `ppo_micro_batch_size_per_gpu` æ˜¯å¦å¤ªå°ï¼ˆå¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼‰
2. `gpu_memory_utilization` æ˜¯å¦å¤ªä½
3. æ˜¯å¦å¼€å¯äº† `param_offload`ï¼ˆåº”è¯¥å…³é—­ï¼‰

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§æŒ‡æ ‡

è¿è¡Œæ—¶å…³æ³¨è¿™äº› timing æŒ‡æ ‡ï¼š

| æŒ‡æ ‡ | è¯´æ˜ | ä¼˜åŒ–æ–¹å‘ |
|------|------|----------|
| `timing_s/gen` | Rollout ç”Ÿæˆæ—¶é—´ | æé«˜ `gpu_memory_utilization`, `max_num_seqs` |
| `timing_s/update_actor` | Actor æ›´æ–°æ—¶é—´ | æé«˜ `ppo_micro_batch_size_per_gpu` |
| `timing_s/old_log_prob` | Log prob è®¡ç®—æ—¶é—´ | æé«˜ `log_prob_micro_batch_size_per_gpu` |
| `timing_s/reward` | Reward è®¡ç®—æ—¶é—´ | é€šå¸¸å¾ˆå¿«ï¼Œæ— éœ€ä¼˜åŒ– |
| `perf/throughput` | Token ååé‡ | ç»¼åˆæŒ‡æ ‡ï¼Œè¶Šé«˜è¶Šå¥½ |

---

## ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ç‰¹æ€§æ±‡æ€»

verl æä¾›äº†å¤šç§æ˜¾å­˜ä¼˜åŒ–ç‰¹æ€§ï¼Œä»¥ä¸‹æŒ‰**æ¨èç¨‹åº¦**æ’åºï¼š

### 1. Gradient Checkpointing (æ¢¯åº¦æ£€æŸ¥ç‚¹) â­â­â­â­â­

**æ•ˆæœ**ï¼šæ˜¾å­˜é™ä½ 30-50%ï¼Œé€Ÿåº¦é™ä½ 10-20%

```yaml
actor_rollout_ref:
  model:
    enable_gradient_checkpointing: True
```

**åŸç†**ï¼šè®­ç»ƒæ—¶ä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ï¼Œç”¨æ—¶é—´æ¢æ˜¾å­˜ã€‚

---

### 2. Sequence Packing (åºåˆ—æ‰“åŒ…) â­â­â­â­â­

**æ•ˆæœ**ï¼šæé«˜ GPU åˆ©ç”¨ç‡ï¼Œå‡å°‘ padding æµªè´¹

```yaml
actor_rollout_ref:
  model:
    use_remove_padding: True
```

**æ”¯æŒæ¨¡å‹**ï¼šQwenã€LLaMAã€Mistralã€Gemma ç­‰

---

### 3. free_cache_engine (é‡Šæ”¾ KV Cache) â­â­â­â­

**æ•ˆæœ**ï¼šè®­ç»ƒæ—¶é‡Šæ”¾ rollout é˜¶æ®µçš„ KV Cacheï¼Œä¸ºè®­ç»ƒè…¾å‡ºæ˜¾å­˜

```yaml
actor_rollout_ref:
  rollout:
    free_cache_engine: True
```

**æ³¨æ„**ï¼šä¸‹æ¬¡ rollout éœ€è¦é‡æ–°é¢„çƒ­ï¼Œæœ‰å°‘é‡å¼€é”€ã€‚

---

### 4. FSDP2 (æ–°ä¸€ä»£åˆ†å¸ƒå¼è®­ç»ƒ) â­â­â­â­

**æ•ˆæœ**ï¼šæ¯” FSDP1 æ˜¾å­˜é™ä½ 7%ï¼Œååæå‡ 1.5%

```yaml
actor_rollout_ref:
  actor:
    strategy: fsdp2
  ref:
    strategy: fsdp2
```

**è¦æ±‚**ï¼šPyTorch 2.1+

---

### 5. Activation Offload (æ¿€æ´»å€¼å¸è½½) â­â­â­

**æ•ˆæœ**ï¼šå°†æ¿€æ´»å€¼å¸è½½åˆ° CPUï¼Œé…åˆ gradient checkpointing ä½¿ç”¨

```yaml
actor_rollout_ref:
  model:
    enable_activation_offload: True
    enable_gradient_checkpointing: True  # å¿…é¡»ä¸€èµ·å¼€å¯
```

**æ³¨æ„**ï¼šä»… FSDP åç«¯æ”¯æŒï¼Œä¼šé™ä½é€Ÿåº¦ã€‚

---

### 6. CPU Offload (å‚æ•°/ä¼˜åŒ–å™¨å¸è½½) â­â­â­

**æ•ˆæœ**ï¼šå¤§å¹…é™ä½æ˜¾å­˜ï¼Œä½†**æ˜¾è‘—é™ä½è®­ç»ƒé€Ÿåº¦**

```yaml
actor_rollout_ref:
  actor:
    fsdp_config:
      param_offload: True       # å‚æ•°å¸è½½åˆ° CPU
      optimizer_offload: True   # ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ° CPU
```

**FSDP2 ä¸“å±**ï¼š
```yaml
actor_rollout_ref:
  actor:
    fsdp_config:
      offload_policy: True  # FSDP2 çš„ CPU offloadï¼Œå…¼å®¹æ¢¯åº¦ç´¯ç§¯
```

**âš ï¸ è­¦å‘Š**ï¼šè¿™æ˜¯æœ€åæ‰‹æ®µï¼Œé€Ÿåº¦ä¼šæ˜æ˜¾å˜æ…¢ï¼

---

### 7. Entropy Chunking (ç†µè®¡ç®—åˆ†å—) â­â­

**æ•ˆæœ**ï¼šé™ä½ logits çš„æ˜¾å­˜å³°å€¼

```yaml
actor_rollout_ref:
  ref:
    entropy_from_logits_with_chunking: True
  actor:
    entropy_checkpointing: True  # è®­ç»ƒæ—¶çš„ç†µé‡è®¡ç®—
```

---

### 8. Dynamic Batch Size (åŠ¨æ€æ‰¹å¤§å°) â­â­

**æ•ˆæœ**ï¼šæŒ‰ token æ•°è€Œéæ ·æœ¬æ•°åˆ†æ‰¹ï¼Œå‡å°‘æ˜¾å­˜æµªè´¹

```yaml
actor_rollout_ref:
  actor:
    use_dynamic_bsz: True
    ppo_max_token_len_per_gpu: 8192  # æ›¿ä»£ micro_batch_size
```

---

### 9. Liger Kernel (é«˜æ€§èƒ½å†…æ ¸) â­â­

**æ•ˆæœ**ï¼šSFT è®­ç»ƒæ•ˆç‡æå‡ï¼Œæ˜¾å­˜ç•¥é™

```yaml
model:
  use_liger: True
```

**å®‰è£…**ï¼š`pip install liger-kernel`

---

### 10. FP8 Rollout â­

**æ•ˆæœ**ï¼šä½¿ç”¨ FP8 è¿›è¡Œæ¨ç†ï¼Œé™ä½ rollout æ˜¾å­˜

éœ€è¦ Hopper æ¶æ„ GPU (H100/H200)ã€‚

---

## ğŸ“‹ æ˜¾å­˜ä¼˜åŒ–é…ç½®æ¨¡æ¿

### æé™çœæ˜¾å­˜é…ç½®ï¼ˆ4x4090 å°æ¨¡å‹ï¼‰

```yaml
actor_rollout_ref:
  model:
    enable_gradient_checkpointing: True
    use_remove_padding: True
    enable_activation_offload: True
  actor:
    ppo_micro_batch_size_per_gpu: 2
    fsdp_config:
      param_offload: False  # ä¸å»ºè®®å¼€ï¼Œå¤ªæ…¢
  rollout:
    gpu_memory_utilization: 0.35
    free_cache_engine: True
    log_prob_micro_batch_size_per_gpu: 8
```

### å¹³è¡¡é…ç½®ï¼ˆæ¨èï¼‰

```yaml
actor_rollout_ref:
  model:
    enable_gradient_checkpointing: True
    use_remove_padding: True
  actor:
    ppo_micro_batch_size_per_gpu: 8
    fsdp_config:
      param_offload: False
  rollout:
    gpu_memory_utilization: 0.5
    free_cache_engine: True
    log_prob_micro_batch_size_per_gpu: 16
```

---

*æ–‡æ¡£ç‰ˆæœ¬: 2025-11-29*

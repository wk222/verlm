# ADPO å¿«é€Ÿå…¥é—¨æŒ‡å—

## 1. éªŒè¯å®‰è£…

é¦–å…ˆéªŒè¯ ADPO æ˜¯å¦æ­£ç¡®å®‰è£…ï¼š

```bash
cd verlm
python examples/test_adpo_installation.py
```

å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œä½ å°±å¯ä»¥å¼€å§‹ä½¿ç”¨ ADPO äº†ï¼

## 2. è¿è¡Œç¬¬ä¸€ä¸ª ADPO è®­ç»ƒ

### æ–¹æ³• 1: ä½¿ç”¨äº¤äº’å¼å¿«é€Ÿå¼€å§‹è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
bash examples/quickstart_adpo.sh
```

è¿™ä¸ªè„šæœ¬ä¼šæä¾›ä¸€ä¸ªèœå•ï¼Œè®©ä½ é€‰æ‹©ä¸åŒçš„ ADPO æ¨¡å¼ã€‚

### æ–¹æ³• 2: ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œ

```bash
# åŸºç¡€ ADPO è®­ç»ƒï¼ˆon-policy æ¨¡å¼ï¼‰
python -m verl.trainer.main_adpo \
    algorithm.adv_estimator=adpo \
    algorithm.anchor_update_mode=on_policy \
    algorithm.num_generations=8 \
    algorithm.tau=0.8
```

### æ–¹æ³• 3: ä½¿ç”¨é¢„å®šä¹‰çš„ç¤ºä¾‹è„šæœ¬

```bash
# GSM8K æ•°æ®é›†ç¤ºä¾‹
bash examples/run_adpo_gsm8k.sh

# å›ºå®šé”šç‚¹æ¨¡å¼ç¤ºä¾‹
bash examples/run_adpo_fixed_anchor.sh

# EMA æ›´æ–°æ¨¡å¼ç¤ºä¾‹
bash examples/run_adpo_ema.sh
```

## 3. ä½¿ç”¨ good_accuracy å¥–åŠ±å‡½æ•°

### å®‰è£…ä¾èµ–

```bash
pip install latex2sympy2_extended math_verify
```

### è¿è¡Œè®­ç»ƒ

```bash
python -m verl.trainer.main_adpo \
    --config-name adpo_trainer \
    algorithm.adv_estimator=adpo \
    custom_reward_function.path=verl/trainer/adpo/reward.py \
    custom_reward_function.name=good_accuracy \
    reward_model.reward_kwargs.ngram_size=4 \
    reward_model.reward_kwargs.max_penalty=-0.5
```

## 4. è‡ªå®šä¹‰é…ç½®

### åˆ›å»º Python é…ç½®æ–‡ä»¶

```python
from examples.adpo_example_config import get_adpo_on_policy_config
from verl.trainer.main_adpo import run_adpo
from omegaconf import OmegaConf

# åŠ è½½åŸºç¡€é…ç½®
config = get_adpo_on_policy_config()

# è‡ªå®šä¹‰é…ç½®
with OmegaConf.open_dict(config):
    config.algorithm.num_generations = 16
    config.algorithm.tau = 0.5
    config.trainer.total_epochs = 50

# è¿è¡Œè®­ç»ƒ
run_adpo(config)
```

### åˆ›å»º YAML é…ç½®æ–‡ä»¶

```yaml
# my_adpo_config.yaml
defaults:
  - adpo_trainer

algorithm:
  adv_estimator: adpo
  num_generations: 8
  tau: 0.8
  anchor_update_mode: on_policy
  use_adaptive_tau: True

trainer:
  project_name: my_project
  experiment_name: my_adpo_experiment
  total_epochs: 30
```

ç„¶åè¿è¡Œï¼š

```bash
python -m verl.trainer.main_adpo --config-name my_adpo_config
```

## 5. ä¸»è¦é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `algorithm.tau` | 0.8 | é”šç‚¹ softmax æ¸©åº¦ |
| `algorithm.anchor_update_mode` | on_policy | é”šç‚¹æ›´æ–°æ¨¡å¼ |
| `algorithm.num_generations` | 8 | æ¯ä¸ª prompt çš„ç”Ÿæˆæ•° |
| `algorithm.use_adaptive_tau` | True | æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”æ¸©åº¦ |
| `algorithm.beta_reward` | 0.5 | å¥–åŠ± softmax æ¸©åº¦ |

## 6. é€‰æ‹©é”šç‚¹æ›´æ–°æ¨¡å¼

```bash
# On-policyï¼ˆæ¨èç”¨äºå¿«é€Ÿå®éªŒï¼‰
python -m verl.trainer.main_adpo algorithm.anchor_update_mode=on_policy

# Fixedï¼ˆæ¨èç”¨äºæœ€å¤§ç¨³å®šæ€§ï¼‰
python -m verl.trainer.main_adpo algorithm.anchor_update_mode=fixed

# EMAï¼ˆæ¨èç”¨äºå¹³æ»‘æ¼”åŒ–ï¼‰
python -m verl.trainer.main_adpo \
    algorithm.anchor_update_mode=ema \
    algorithm.ema_alpha=0.99

# KL-triggeredï¼ˆæ¨èç”¨äºè‡ªé€‚åº”åœºæ™¯ï¼‰
python -m verl.trainer.main_adpo \
    algorithm.anchor_update_mode=kl_triggered \
    algorithm.kl_threshold=0.1
```

## 7. ç›‘æ§è®­ç»ƒ

### æŸ¥çœ‹æ—¥å¿—

```bash
# è®­ç»ƒè¿‡ç¨‹ä¼šè¾“å‡ºåˆ°ç»ˆç«¯
# æ—¥å¿—ä¹Ÿä¼šä¿å­˜åˆ°è¾“å‡ºç›®å½•
```

### ä½¿ç”¨ WandBï¼ˆå¦‚æœé…ç½®ï¼‰

```bash
# åœ¨é…ç½®ä¸­å¯ç”¨ wandb
trainer.logger=["console", "wandb"]
```

### å…³é”®æŒ‡æ ‡

- `adpo/anchor_kl`: ç­–ç•¥ä¸é”šç‚¹çš„ KL æ•£åº¦
- `adpo/mean_tau`: å¹³å‡æ¸©åº¦å€¼
- `adpo/loss`: ADPO æŸå¤±
- `reward`: å¹³å‡å¥–åŠ±

## 8. å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1: æ‰¹å¤§å°è­¦å‘Š

```
Warning: per_device_train_batch_size (12) is not divisible by num_generations (8)
```

**è§£å†³**: è®¾ç½®æ‰¹å¤§å°ä¸º `num_generations` çš„å€æ•°

```bash
python -m verl.trainer.main_adpo \
    algorithm.num_generations=8 \
    trainer.per_device_train_batch_size=16  # 16 = 8 * 2
```

### é—®é¢˜ 2: good_accuracy å¯¼å…¥é”™è¯¯

```
ImportError: No module named 'latex2sympy2_extended'
```

**è§£å†³**: å®‰è£…ä¾èµ–

```bash
pip install latex2sympy2_extended math_verify
```

### é—®é¢˜ 3: Ray åˆå§‹åŒ–å¤±è´¥

```
RuntimeError: Ray is not initialized
```

**è§£å†³**: ç¡®ä¿ Ray é…ç½®æ­£ç¡®

```yaml
ray_kwargs:
  ray_init:
    num_cpus: null  # è‡ªåŠ¨æ£€æµ‹
```

## 9. ä¸‹ä¸€æ­¥

- ğŸ“– **è¯¦ç»†æ–‡æ¡£**: `verl/trainer/adpo/README.md`
- ğŸ”§ **é…ç½®ç¤ºä¾‹**: `examples/adpo_example_config.py`
- ğŸ“Š **é›†æˆæ€»ç»“**: `ADPO_INTEGRATION_SUMMARY.md`

## 10. è·å–å¸®åŠ©

1. é˜…è¯» README: `verl/trainer/adpo/README.md`
2. æŸ¥çœ‹ç¤ºä¾‹è„šæœ¬: `examples/run_adpo_*.sh`
3. è¿è¡Œæµ‹è¯•: `python examples/test_adpo_installation.py`
4. æäº¤ Issue

---

**ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€


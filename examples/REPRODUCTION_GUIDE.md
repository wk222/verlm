# ADPO å®éªŒå¤ç°æŒ‡å—

## å¤ç° Qwen3-1.7B on MATH Dataset

æœ¬æŒ‡å—å±•ç¤ºå¦‚ä½•ä½¿ç”¨ VERL å¤ç° Open-R1 ADPO baseline å®éªŒã€‚

### åŸå§‹é…ç½®æ¥æº

åŸºäº `OPENR1_ADPO-VERSION/recipes/Qwen3/adpo/config_qwen3-1_6b.yaml`

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. è¿›å…¥ verlm ç›®å½•
cd verlm

# 2. å®‰è£…ä¾èµ–
pip install latex2sympy2_extended math_verify

# 3. éªŒè¯å®‰è£…
python examples/test_adpo_installation.py
```

### å¿«é€Ÿå¯åŠ¨

```bash
# ä½¿ç”¨é¢„é…ç½®çš„è„šæœ¬
bash examples/reproduce_qwen3_math_adpo.sh
```

### è¯¦ç»†é…ç½®

#### é…ç½®æ–‡ä»¶: `verl/trainer/config/adpo_qwen3_math.yaml`

**æ ¸å¿ƒé…ç½®å¯¹æ¯”**:

| é…ç½®é¡¹ | åŸå§‹ TRL-ADPO | VERL-ADPO |
|--------|---------------|-----------|
| æ¨¡å‹ | Qwen/Qwen3-1.7B | âœ“ ç›¸åŒ |
| æ•°æ®é›† | MATH-lighteval-level_3 | âœ“ ç›¸åŒ |
| num_generations | 8 | âœ“ ç›¸åŒ |
| tau | 0.8 | âœ“ ç›¸åŒ |
| beta_reward | 0.5 | âœ“ ç›¸åŒ |
| anchor_update_mode | on_policy | âœ“ ç›¸åŒ |
| use_adaptive_tau | True | âœ“ ç›¸åŒ |
| adaptive_tau_alpha | 1.0 | âœ“ ç›¸åŒ |
| adaptive_tau_min | 0.1 | âœ“ ç›¸åŒ |
| learning_rate | 1.5e-5 | âœ“ ç›¸åŒ |
| gradient_accumulation_steps | 16 | âœ“ ç›¸åŒ |
| per_device_train_batch_size | 8 | âœ“ ç›¸åŒ |
| num_train_epochs | 2 | âœ“ ç›¸åŒ |
| vLLM | colocate | âœ“ ç›¸åŒ |
| reward_func | good_accuracy | âœ“ ç›¸åŒ |

### æ‰‹åŠ¨è¿è¡Œ

```bash
# åŸºç¡€å‘½ä»¤
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math

# è‡ªå®šä¹‰è¾“å‡ºç›®å½•
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.default_local_dir=my_output_dir

# ä¿®æ”¹è®­ç»ƒå‚æ•°
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    algorithm.num_generations=16 \
    algorithm.tau=1.0 \
    trainer.per_device_train_batch_size=16
```

### é…ç½®è¯´æ˜

#### 1. æ¨¡å‹é…ç½®

```yaml
actor_rollout_ref:
  model:
    path: Qwen/Qwen3-1.7B
  actor:
    model_init_kwargs:
      torch_dtype: bfloat16
      attn_implementation: flash_attention_2
```

#### 2. vLLM é…ç½®

```yaml
rollout:
  use_vllm: true
  vllm_mode: colocate
  vllm_enable_sleep_mode: true
  vllm_gpu_memory_utilization: 0.4
```

#### 3. ADPO ç®—æ³•é…ç½®

```yaml
algorithm:
  adv_estimator: adpo
  num_generations: 8
  tau: 0.8
  anchor_update_mode: on_policy
  use_adaptive_tau: true
  adaptive_tau_alpha: 1.0
  adaptive_tau_min: 0.1
  beta_reward: 0.5
```

#### 4. å¥–åŠ±å‡½æ•°é…ç½®

```yaml
custom_reward_function:
  path: verl/trainer/adpo/reward.py
  name: good_accuracy

reward_model:
  reward_kwargs:
    ngram_size: 4
    max_penalty: -0.5
    penalty_scale_factor: 0.1
```

### å¤šGPUè®­ç»ƒ

```bash
# 8 GPU è®­ç»ƒï¼ˆé»˜è®¤ï¼‰
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
bash examples/reproduce_qwen3_math_adpo.sh

# 4 GPU è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0,1,2,3
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.n_gpus_per_node=4

# å• GPU æµ‹è¯•
export CUDA_VISIBLE_DEVICES=0
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.n_gpus_per_node=1 \
    trainer.per_device_train_batch_size=2
```

### ç›‘æ§è®­ç»ƒ

#### WandB é…ç½®

```yaml
wandb_config:
  project: open-r1-ADPO
  name: qwen3-1.7b-adpo-baseline
  group: qwen3_adpo_baseline
  tags: [adpo, qwen3, math]
```

æŸ¥çœ‹æŒ‡æ ‡ï¼š
```bash
# ç™»å½• WandB
wandb login

# è®­ç»ƒä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° WandB
```

#### æœ¬åœ°æ—¥å¿—

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f data/Qwen3-1.7B-Open-R1-ADPO/logs/train.log

# æŸ¥çœ‹æ£€æŸ¥ç‚¹
ls data/Qwen3-1.7B-Open-R1-ADPO/checkpoint-*
```

### æ¢å¤è®­ç»ƒ

```bash
# ä»æ£€æŸ¥ç‚¹æ¢å¤
bash examples/reproduce_qwen3_math_adpo.sh \
    trainer.resume_from_checkpoint=data/Qwen3-1.7B-Open-R1-ADPO/checkpoint-1000
```

### è¯„ä¼°æ¨¡å‹

```bash
# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.do_eval=true \
    trainer.do_train=false
```

### å¸¸è§é—®é¢˜

#### Q1: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: OOM (Out of Memory) é”™è¯¯

**è§£å†³**:
```bash
# å‡å°æ‰¹å¤§å°
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.per_device_train_batch_size=4 \
    algorithm.num_generations=4

# æˆ–å¢åŠ æ¢¯åº¦ç´¯ç§¯
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.gradient_accumulation_steps=32
```

#### Q2: vLLM åˆå§‹åŒ–å¤±è´¥

**ç—‡çŠ¶**: vLLM ç›¸å…³é”™è¯¯

**è§£å†³**:
```bash
# é™ä½ GPU åˆ©ç”¨ç‡
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    rollout.vllm_gpu_memory_utilization=0.3

# æˆ–ç¦ç”¨ sleep mode
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    rollout.vllm_enable_sleep_mode=false
```

#### Q3: æ‰¹å¤§å°è­¦å‘Š

**ç—‡çŠ¶**: Batch size not divisible by num_generations

**è§£å†³**: ç¡®ä¿æ‰¹å¤§å°æ˜¯ `num_generations` çš„å€æ•°
```bash
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    trainer.per_device_train_batch_size=16 \
    algorithm.num_generations=8
```

### å®éªŒå˜ä½“

#### å˜ä½“ 1: Fixed Anchor

```bash
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    algorithm.anchor_update_mode=fixed \
    algorithm.tau=1.0 \
    trainer.experiment_name=qwen3-adpo-fixed-anchor
```

#### å˜ä½“ 2: EMA Anchor

```bash
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    algorithm.anchor_update_mode=ema \
    algorithm.ema_alpha=0.99 \
    trainer.experiment_name=qwen3-adpo-ema
```

#### å˜ä½“ 3: æ›´å¤šç”Ÿæˆæ•°

```bash
python -m verl.trainer.main_adpo \
    --config-name adpo_qwen3_math \
    algorithm.num_generations=16 \
    trainer.per_device_train_batch_size=16 \
    trainer.experiment_name=qwen3-adpo-gen16
```

### ç»“æœå¯¹æ¯”

é¢„æœŸç»“æœï¼ˆåŸºäºåŸå§‹ TRL-ADPO å®éªŒï¼‰ï¼š

| Metric | Epoch 1 | Epoch 2 |
|--------|---------|---------|
| Accuracy | ~0.35 | ~0.45 |
| Mean Reward | ~0.40 | ~0.50 |
| Mean Tau | ~0.6 | ~0.5 |

**æ³¨æ„**: å®é™…ç»“æœå¯èƒ½å› éšæœºç§å­å’Œç¡¬ä»¶è€Œç•¥æœ‰ä¸åŒã€‚

### ä¸‹ä¸€æ­¥

1. **æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£**: `verl/trainer/adpo/README.md`
2. **å°è¯•å…¶ä»–é…ç½®**: `examples/adpo_example_config.py`
3. **è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°**: å‚è€ƒ `verl/trainer/adpo/reward.py`

### å¼•ç”¨

å¦‚æœä½¿ç”¨æ­¤å¤ç°è„šæœ¬ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{zixian2025adpoanchoreddirectpreference,
    title={ADPO: Anchored Direct Preference Optimization}, 
    author={Wang Zixian},
    year={2025},
    eprint={2510.18913},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2510.18913}, 
}
```

---

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸš€


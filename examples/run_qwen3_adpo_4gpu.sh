#!/bin/bash
# Reproduce Open-R1 ADPO Baseline - Qwen3-1.7B on WZX MATH Dataset
# Optimized for 4x4090 (24GB VRAM each)

set -e  # Exit on error

echo "=========================================="
echo "ADPO Reproduction - Qwen3 on WZX MATH (4x4090)"
echo "=========================================="
echo ""

# Check if we're in the verlm directory
if [ ! -d "verl/trainer/adpo" ]; then
    echo "âŒ Error: Please run this script from the verlm/ directory."
    echo "   Current directory: $(pwd)"
    exit 1
fi

# Set environment variables
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
export CUDA_VISIBLE_DEVICES=0,1,2,3  # 4 GPUs

# Configuration
CONFIG_NAME="adpo_qwen3_math_hybrid" # Use our new hybrid config
OUTPUT_DIR="data/Qwen3-1.7B-Open-R1-ADPO-WZX"
DATA_DIR="data/math_wzx"
N_GPUS=4

echo "Configuration:"
echo "  - Config: ${CONFIG_NAME}"
echo "  - Output: ${OUTPUT_DIR}"
echo "  - Data: ${DATA_DIR}"
echo "  - GPUs: ${CUDA_VISIBLE_DEVICES} (${N_GPUS} GPUs)"
echo ""

# Download and preprocess WZX MATH dataset if not exists
if [ ! -f "${DATA_DIR}/train.parquet" ]; then
    echo "ğŸ“¥ Downloading and preprocessing WZX MATH dataset..."
    python3 examples/data_preprocess/math_wzx_dataset.py \
        --local_save_dir ${DATA_DIR}
    echo ""
else
    echo "âœ… WZX MATH dataset already exists at ${DATA_DIR}"
    echo ""
fi

# Run ADPO training
echo "ğŸš€ Starting ADPO training..."
echo ""

# Batch Size Calculation for 4x4090 (24GB VRAM each):
# ============================================================
# çº¦æŸ: train_batch_size >= ppo_mini_batch_size
# ============================================================
# - train_batch_size: æ¯ä¸ªè®­ç»ƒæ­¥éª¤çš„æç¤ºæ•°é‡
# - ppo_mini_batch_size: PPO æ›´æ–°çš„ mini-batch å¤§å° (å¿…é¡» <= train_batch_size)
# - ppo_micro_batch_size_per_gpu: æ¯ GPU çš„å¾®æ‰¹æ¬¡å¤§å° (ç”¨äºæ¢¯åº¦ç´¯ç§¯)
# - rollout.n: æ¯ä¸ªæç¤ºç”Ÿæˆçš„å“åº”æ•°é‡
#
# é…ç½®è¯´æ˜:
# - train_batch_size=256: æ¯æ­¥ä½¿ç”¨ 256 ä¸ªæç¤º (å¢å¤§ä»¥æå‡åå)
# - ppo_mini_batch_size=128: PPO æ¯æ¬¡æ›´æ–°ç”¨ 128 ä¸ªæ ·æœ¬
# - ppo_micro_batch_size_per_gpu=8: æ¯ GPU å¤„ç† 8 ä¸ªæ ·æœ¬ (æ¢¯åº¦ç´¯ç§¯ = 128/(4*8)=4)
# - rollout.n=4: æ¯æç¤ºç”Ÿæˆ 4 ä¸ªå“åº” (æ€»åºåˆ—æ•° = 256*4=1024)
# - max_prompt_length=1024: æ”¯æŒé•¿ prompt (æœ‰äº›æ•°æ® 800+ tokens)
# - max_response_length=1280: æ”¯æŒ 1200+ çš„ response
# - truncation=left: è¶…é•¿ prompt ä»å·¦è¾¹æˆªæ–­ (ä¿ç•™æœ€è¿‘å†…å®¹)
# - gpu_memory_utilization=0.7: æå‡ GPU æ˜¾å­˜åˆ©ç”¨ç‡

python -m verl.trainer.main_adpo \
    --config-name ${CONFIG_NAME} \
    data.train_files=${DATA_DIR}/train.parquet \
    data.train_batch_size=256 \
    data.max_prompt_length=1024 \
    data.max_response_length=1280 \
    data.truncation=left \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.7 \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16 \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    trainer.n_gpus_per_node=${N_GPUS} \
    trainer.default_local_dir=${OUTPUT_DIR} \
    trainer.experiment_name=qwen3-1.7b-adpo-wzx-4gpu \
    wandb_config.name=qwen3-1.7b-adpo-wzx-4gpu \
    "$@"  # Pass any additional arguments

echo ""
echo "=========================================="
echo "âœ… ADPO Training Complete!"
echo "=========================================="

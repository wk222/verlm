# ADPO Training Configuration - Qwen3 on WZX MATH Dataset (Hybrid mode - 4 GPU)
# Based on adpo_trainer.yaml with customizations for 4-GPU setup
# ADPO does not require a critic model

# Specify default configs from base templates
defaults:
  - actor@actor_rollout_ref.actor: dp_actor
  - data@data: legacy_data
  - rollout@actor_rollout_ref.rollout: rollout
  - model@actor_rollout_ref.model: hf_model
  - critic@critic: dp_critic
  - reward_model@reward_model: dp_reward_model
  - algorithm@algorithm.rollout_correction: rollout_correction
  - _self_

# Critic configuration (disabled for ADPO)
critic:
  enable: False

# Data configuration
data:
  train_files: $DATA_DIR/train.parquet
  val_files: $DATA_DIR/train.parquet
  prompt_key: prompt
  max_prompt_length: 1024
  max_response_length: 1280
  train_batch_size: 128
  val_batch_size: 64
  truncation: left
  shuffle: true  # 必须 shuffle，否则训练不稳定
  dataloader_num_workers: 0
  return_raw_input_ids: False
  return_raw_chat: False

# Actor, rollout, and reference model configuration
actor_rollout_ref:
  hybrid_engine: True
  model:
    path: Qwen/Qwen3-1.7B
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: True
    enable_activation_offload: False
    use_remove_padding: True
  actor:
    strategy: fsdp2
    ppo_mini_batch_size: 60         # [修改] 从 64 改为 60 (能被 6 整除)
    ppo_micro_batch_size: null
    ppo_micro_batch_size_per_gpu: 6  # 与 GSPO 一致
    use_dynamic_bsz: False
    use_kl_loss: False
    policy_loss:
      loss_mode: adpo
      num_generations: 6              # [修改] 从 8 改为 6，以支持 Plackett-Luce
    optim:
      lr: 1e-6
    fsdp_config:
      wrap_policy:
        min_num_params: 0
      param_offload: False
      optimizer_offload: False
      dtype: bfloat16
      model_dtype: bfloat16
  rollout:
    name: vllm
    mode: sync
    temperature: 1.0
    top_k: -1
    top_p: 1.0
    n: 6                            # [修改] 从 8 改为 6
    gpu_memory_utilization: 0.55
    tensor_model_parallel_size: 1
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: 16
    max_num_seqs: 192
    ignore_eos: False
    enforce_eager: False
    enable_chunked_prefill: True
    enable_prefix_caching: True
    free_cache_engine: True
    do_sample: True

# Algorithm configuration for ADPO
algorithm:
  adv_estimator: adpo
  
  # 统一定义 num_generations，避免命令行重复写
  num_generations: 6                # [修改] 从 8 改为 6
  
  # Required by base trainer (not used by ADPO, but needed for compatibility)
  gamma: 1.0
  lam: 1.0
  
  # ADPO does not use KL penalty (on-policy mode)
  use_kl_in_reward: False
  kl_ctrl:
    type: fixed
    kl_coef: 0.0
  
  # ============================================================
  # ADPO Core Hyperparameters
  # ============================================================
  # Loss变体：
  #   "pairwise"             : DPO风格 -log σ(u_w - u_l) ⭐推荐
  #   "plackett_luce"        : P-L模型/ListMLE，完整排序对齐
  #   "plackett_luce_approx" : P-L近似版（只看top-k）
  #   "direct"               : -q·u + logsumexp(u)
  #   "infonce"              : -u_best + logsumexp(u)  
  #   "softmax"              : 原始ADPO
  #   "scaled"               : 原始ADPO × grad_scale_factor
  # ============================================================
  
  loss_variant: plackett_luce     # [修改] 启用 Plackett-Luce，最强排序信号
  tau: 0.1                        # [优化] P-L 梯度强，稍微调大 tau (0.05->0.1) 增加鲁棒性
  beta_reward: 0.05               # [优化] 保持锐利，增强排序信号
  clip_anchored_score: 10.0       # anchored_scores裁剪范围
  clip_log_ratio: 5.0             # log_ratio裁剪
  use_length_normalization: True  # 长度归一化
  grad_scale_factor: 20.0         # scaled变体使用
  logit_reg_coef: 0.01            # Z-Loss 系数，防止 Logits 爆炸 (建议 0.001~0.01)
  use_q_center: True              # Q-加权中心化，消除平移不变性 (建议开启)
  
  # Plackett-Luce 稳定性参数
  pl_top_k: 6                     # [优化] 使用全量 Top-6 (等于 num_generations)
  pl_temperature: 1.0             # >1 使分布更平滑
  pl_label_smoothing: 0.1         # 软化排序目标 (推荐 0.1)
  
  # Poly-Loss (仅对 Plackett-Luce 生效)
  use_poly_loss: True
  poly_epsilon: 1.0
  
  # Softmax 变体参数 (u = ℓ - A·ℓ_ref - B·q)
  softmax_coef_A: 1.0
  softmax_coef_B: 0.5
  
  # ============================================================
  # 自适应温度策略
  # ============================================================
  # 公式：τ_eff = τ × (1 + α·h_static + β·c·(-s))
  # - 高置信+高奖励 → 降温 → 追高
  # - 高置信+低奖励 → 升温 → 刹车
  use_adaptive_tau: True          # 开启自适应温度
  adaptive_tau_alpha: 0.2         # 静态防御系数
  adaptive_tau_beta: 0.5          # 追高/刹车系数
  adaptive_tau_min: 0.2           # τ下限
  adaptive_tau_max: 1.5           # τ上限
  vocab_size: 151936              # Qwen3 vocab size
  
  # ============================================================
  # 梯度稳定化
  # ============================================================
  grad_clip_value: 0.0            # loss级别裁剪，0=不裁剪
  
  # Advantage Estimation
  scale_rewards: False
  drop_all_failed_prompts: False
  
  # Gradient Accumulation
  global_batch_size: 32

# Trainer configuration
trainer:
  total_epochs: 2  # 与 GSPO 保持一致
  project_name: open-r1-ADPO
  experiment_name: qwen3-1.7b-adpo-baseline
  logger: ['wandb', 'console']
  nnodes: 1
  n_gpus_per_node: 4
  save_freq: -1  # keep -1 (disabled) because we will use save_each_epoch for epoch-level checkpoint
  save_each_epoch: true  # custom flag: save checkpoint at end of each epoch
  test_freq: -1
  val_before_train: False
  seed: 42
  default_hdfs_dir: null
  default_local_dir: null
  device: cuda
  balance_batch: True
  total_training_steps: null
  log_val_generations: 0
  rollout_data_dir: null
  validation_data_dir: null
  esi_redundant_time: 0
  resume_mode: auto
  resume_from_path: null
  val_only: False
  critic_warmup: 0
  del_local_ckpt_after_load: False
  max_actor_ckpt_to_keep: null
  max_critic_ckpt_to_keep: null
  ray_wait_register_center_timeout: 300
  use_legacy_worker_impl: auto
  bf16: True
  fp16: False
  push_to_hub: true  # enable upload to HuggingFace Hub
  hub_model_id: wzx111/Qwen3-1.7B-ADPO-math  # change to your HF repo (must exist or will be created)
  hub_private: true  # create private repo if not exists
  hub_final_upload: true  # push final checkpoint (latest global_step)
  hub_push_each_epoch: true  # push checkpoint at end of each epoch
  hub_commit_message: "VERL ADPO checkpoint"

# Custom reward function (good_accuracy from ADPO)
custom_reward_function:
  path: verl/trainer/adpo/reward.py
  name: good_accuracy
  reward_kwargs:
    ngram_size: 4
    max_penalty: -0.5
    penalty_scale_factor: 0.5
    length_reward_scale: 2000.0
    length_reward_max: 0.1
    repetition_threshold: 0.3
    repetition_threshold_penalty: -0.5

# Reward model configuration
# ADPO uses custom_reward_function (good_accuracy), no external RM needed
reward_model:
  enable: False

# WandB logging configuration
wandb_config:
  project: open-r1-ADPO
  entity: null
  name: qwen3-1.7b-adpo-baseline
  group: qwen3_adpo
  tags:
    - adpo
    - qwen3
    - math
    - reasoning

# Ray distributed computing configuration
ray_kwargs:
  ray_init:
    runtime_env:
      env_vars:
        TOKENIZERS_PARALLELISM: "false"
        NCCL_DEBUG: "WARN"
        VLLM_LOGGING_LEVEL: "WARNING"
    num_cpus: null
  timeline_json_file: null

# Transfer queue configuration
transfer_queue:
  enable: False

# Global profiler configuration
global_profiler:
  tool: none
  steps: []
  profile_continuous_steps: false

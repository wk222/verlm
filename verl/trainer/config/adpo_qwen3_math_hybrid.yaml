data:
  train_files: $DATA_DIR/train.parquet
  val_files: $DATA_DIR/test.parquet
  prompt_key: problem
  max_prompt_length: 700
  max_response_length: 1200
  train_batch_size: 8
  val_batch_size: 8
  return_raw_input_ids: False  # For vLLM rollout
  return_raw_chat: False

actor_rollout_ref:
  hybrid_engine: True
  model:
    path: Qwen/Qwen3-1.7B
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: True
    use_remove_padding: True
  actor:
    strategy: fsap  # FSDP
    ppd: 1
    mpd: 1
    ppo_mini_batch_size: 256
    ppo_micro_batch_size: null
    optim:
      lr: 1.5e-5
    fsdp_config:
      wrap_policy:
        # Transformer layer wrapper
        min_num_params: 0
      param_offload: False
      grad_offload: False
      optimizer_offload: False
  rollout:
    temperature: 1.0
    top_k: -1 # 0 in some configs, -1 usually means disabled or all
    top_p: 1.0
    n: 8 # num_generations
    gpu_memory_utilization: 0.4
    ignore_eos: False
    enforce_eager: False
    free_cache_after_step: True
    do_sample: True
  ref:
    fsdp_config:
      param_offload: True
      grad_offload: True
      optimizer_offload: True
    log_prob_micro_batch_size: 4 # Adjust based on VRAM

algorithm:
  adv_estimator: adpo
  kl_ctrl:
    type: fixed
    kl_coef: 0.0 # beta_anchor_kl
  
  # ADPO Hyperparameters
  tau: 1.0
  use_adaptive_tau: True
  adaptive_tau_alpha: 0.7
  adaptive_tau_min: 0.3
  adaptive_tau_beta: 0.5 # New param we added
  adaptive_tau_max: 5.0 # New param we added
  beta_reward: 0.8
  drop_all_failed_prompts: False
  
  # Gradient Accumulation
  global_batch_size: 64 # 8 * 8 = 64 (approx, adjust based on num_gpus)

trainer:
  total_epochs: 2
  project_name: open-r1-ADPO
  experiment_name: qwen3-1.7b-adpo-baseline
  logger: ['wandb', 'console']
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: -1
  test_freq: 1
  val_before_train: False
  default_hdfs_dir: null
  default_local_dir: null

reward_model:
  enable: True
  reward_manager: custom
  reward_kwargs:
    mode: math_adpo # Points to our new reward file
    ngram_size: 3
    max_penalty: -1.0
    penalty_scale_factor: 0.1
    length_reward_scale: 2000.0
    length_reward_max: 0.1
    repetition_threshold: 0.3
    repetition_threshold_penalty: -0.5

wandb_config:
  project: open-r1-ADPO
  entity: null
  name: qwen3-1.7b-adpo-baseline
  group: qwen3_adpo
  tags:
    - adpo
    - qwen3
    - math
    - reasoning

ray_kwargs:
  ray_init:
    runtime_env:
      env_vars:
        TOKENIZERS_PARALLELISM: "false"
        NCCL_DEBUG: "WARN"
        VLLM_LOGGING_LEVEL: "WARNING"
    num_cpus: null
  timeline_json_file: null

transfer_queue:
  enable: False
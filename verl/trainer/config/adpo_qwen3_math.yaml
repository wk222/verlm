# ADPO Training Configuration - Qwen3 on MATH Dataset
# Based on Open-R1 ADPO baseline configuration
# Dataset: MATH-lighteval-level_3

defaults:
  - adpo_trainer

# Model configuration
actor_rollout_ref:
  model:
    path: Qwen/Qwen3-1.7B
    # use_shm: false
  
  actor:
    # Training strategy: fsdp or fsdp2
    strategy: fsdp
    
    # Batch size configuration (reduced for memory efficiency)
    ppo_micro_batch_size_per_gpu: 4
    
    # FSDP configuration
    fsdp_config:
      dtype: bfloat16
      model_dtype: bfloat16  # Model initialization dtype
    
    # Optimizer configuration
    optim:
      lr: 1.5e-5
    
    # Policy loss configuration - IMPORTANT: Use ADPO loss for ADPO algorithm
    policy_loss:
      loss_mode: adpo
      
    # Use KL loss
    use_kl_loss: false
  
  rollout:
    # Rollout engine: vllm/hf/sglang
    name: vllm
    
    # vLLM configuration
    mode: async
    
    # GPU memory utilization for vLLM (with 2-GPU tensor parallel)
    gpu_memory_utilization: 0.85
    
    # Model parallelism (2 GPUs for vLLM rollout)
    tensor_model_parallel_size: 2
    
    # Batch size for log probability computation (reduced for memory)
    log_prob_micro_batch_size_per_gpu: 4
    
    # Generation parameters
    temperature: 1.0
    top_p: 1.0
    top_k: -1
    prompt_length: 1024  # Must match max_prompt_length
    response_length: 1024  # Must match max_response_length

# Data configuration
data:
  # Dataset - using local preprocessed MATH Level 3 dataset
  train_files:
    - /root/data/math_level3/train.parquet
  val_files:
    - /root/data/math_level3/test.parquet
  
  # Prompt column (must match the column name in parquet files)
  prompt_key: prompt
  
  # Maximum sequence lengths (unified to 1024 for MATH dataset)
  max_prompt_length: 1024
  max_response_length: 1024
  
  # System prompt (Note: instruction is already in the question)
  system_prompt: null
  
  # Shuffling
  shuffle: false
  
  # Trust remote code
  trust_remote_code: true

# ADPO Algorithm configuration (on-policy mode for memory efficiency)
algorithm:
  _target_: verl.trainer.config.AlgoConfig
  
  # Advantage estimator
  adv_estimator: adpo
  
  # Number of generations per prompt
  num_generations: 8
  
  # ADPO Core Parameters (on-policy mode uses old_log_prob as anchor)
  tau: 0.8
  beta_anchor_kl: 0.0
  
  # Adaptive Temperature
  use_adaptive_tau: true
  adaptive_tau_alpha: 1.0
  adaptive_tau_min: 0.1
  
  # Q computation
  beta_reward: 0.5
  
  # Reward scaling
  scale_rewards: group
  
  # Drop failed prompts
  drop_all_failed_prompts: false
  
  # KL penalty (disabled)
  use_kl_in_reward: false
  kl_penalty: kl

# Reward function configuration
custom_reward_function:
  path: verl/trainer/adpo/reward.py
  name: good_accuracy

reward_model:
  enable: false
  reward_kwargs: {}

# Trainer configuration
trainer:
  # Project and experiment name
  project_name: open-r1-ADPO
  experiment_name: qwen3-1.7b-adpo-baseline
  
  # Training parameters
  total_epochs: 2
  total_training_steps: null
  
  # Batch size (reduced to accommodate memory constraints)
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  
  # Gradient accumulation
  gradient_accumulation_steps: 16
  
  # Gradient checkpointing
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  
  # Mixed precision
  bf16: true
  fp16: false
  
  # Learning rate scheduler
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  
  # Evaluation
  do_eval: true
  eval_strategy: epoch
  
  # Saving
  save_strategy: epoch
  save_total_limit: 2
  save_freq: 1
  
  # Logging
  logger: ["console", "wandb"]
  log_completions: true
  log_level: info
  logging_first_step: true
  logging_steps: 1
  
  # Output directory
  default_local_dir: data/Qwen3-1.7B-Open-R1-ADPO
  
  # Hub
  push_to_hub: false
  hub_model_id: null
  
  # Seed
  seed: 42
  
  # Dataloader
  dataloader_drop_last: true
  dataloader_num_workers: 0
  
  # Nodes and GPUs
  nnodes: 1
  n_gpus_per_node: 4  # 4 GPUs for reproduction
  
  # Device
  device: cuda
  
  # Resume settings
  resume_mode: auto  # auto/disable/resume_path
  resume_from_path: null
  
  # Validation settings
  val_before_train: true
  val_only: false
  test_freq: -1  # Validation frequency (in training iterations), -1 to disable
  
  # Critic warmup (number of iterations to warm up the critic before updating policy)
  critic_warmup: 0
  
  # ESI redundant time
  esi_redundant_time: 0
  
  # Checkpoint settings
  default_hdfs_dir: null
  del_local_ckpt_after_load: false
  max_actor_ckpt_to_keep: null
  max_critic_ckpt_to_keep: null
  
  # Ray settings
  ray_wait_register_center_timeout: 300
  
  # Worker implementation mode
  use_legacy_worker_impl: auto
  
  # Balance batch
  balance_batch: true
  
  # Log validation generations
  log_val_generations: 0
  
  # Rollout data directory (null to disable)
  rollout_data_dir: null
  
  # Validation data directory (null to disable)
  validation_data_dir: null

# WandB configuration
wandb_config:
  project: open-r1-ADPO
  entity: null
  name: qwen3-1.7b-adpo-baseline
  group: qwen3_adpo_baseline
  tags:
    - adpo
    - qwen3
    - math
    - reasoning

# Ray configuration
ray_kwargs:
  ray_init:
    runtime_env:
      env_vars:
        TOKENIZERS_PARALLELISM: "false"
        NCCL_DEBUG: "WARN"
        VLLM_LOGGING_LEVEL: "WARNING"
    num_cpus: null

# Transfer queue (disabled)
transfer_queue:
  enable: false

# Global profiler (disabled)
global_profiler:
  tool: none
  steps: []
  profile_continuous_steps: false
  save_path: "outputs/profile"

